{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп» с BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Импорт-библиотек,-настройка,-константы\" data-toc-modified-id=\"Импорт-библиотек,-настройка,-константы-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Импорт библиотек, настройка, константы</a></span></li><li><span><a href=\"#Загрузка-датасета\" data-toc-modified-id=\"Загрузка-датасета-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Загрузка датасета</a></span></li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Подготовка-эмбеддингов\" data-toc-modified-id=\"Подготовка-эмбеддингов-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Подготовка эмбеддингов</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>TF-IDF</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия\" data-toc-modified-id=\"Логистическая-регрессия-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Логистическая регрессия</a></span></li><li><span><a href=\"#Catboost\" data-toc-modified-id=\"Catboost-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Catboost</a></span></li></ul></li><li><span><a href=\"#BERT\" data-toc-modified-id=\"BERT-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>BERT</a></span><ul class=\"toc-item\"><li><span><a href=\"#Логистическая-регрессия-c-BERT\" data-toc-modified-id=\"Логистическая-регрессия-c-BERT-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Логистическая регрессия c BERT</a></span></li></ul></li></ul></li><li><span><a href=\"#Тестирование-моделей.\" data-toc-modified-id=\"Тестирование-моделей.-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Тестирование моделей.</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт библиотек, настройка, константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from torch) (3.10.0.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (4.9.2)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: packaging in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied: requests in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (4.6.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from transformers) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from huggingface-hub==0.0.12->transformers) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: click in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from sacremoses->transformers) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: catboost in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from catboost) (1.18.3)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from catboost) (0.25.1)\n",
      "Requirement already satisfied: graphviz in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from catboost) (0.17)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from catboost) (3.1.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from catboost) (1.4.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from catboost) (4.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from pandas>=0.24.0->catboost) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from matplotlib->catboost) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from matplotlib->catboost) (0.10.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\ivan\\.conda\\envs\\praktikum\\lib\\site-packages (from plotly->catboost) (1.3.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement warnings (from versions: none)\n",
      "ERROR: No matching distribution found for warnings\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install catboost\n",
    "!pip install warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "\n",
    "from tqdm import notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    " \n",
    "tqdm.pandas()\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RANDOM_STATE = 12345\n",
    "\n",
    "#Максимальное количество эмбеддингов, и размер батча для их получения.\n",
    "BERT_SAMPLES = 2000\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "#Путь к датасету\n",
    "PATH_TO_DATASET = 'C:\\\\Users\\\\Ivan\\\\Downloads\\\\Yandex_Practicum\\\\text\\\\toxic_comments.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем решать задачу двумя способами, 'классичесским' через TF-IDF и с помощью эмбеддингов предобученной модели *BERT*.    \n",
    "\n",
    "Выделим для последней сразу выборку. Так как модель *BERT* достаточно тяжёлая, и эмбединги создаются долго, попробуем ограничиться выборкой в 2000 строк (500 строк - запас для дальнейшей фильтрации последовательностей токенов > 512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Ahh shut the fuck up you douchebag sand nigger \\n\\nGo blow up some more people you muslim piece of shit. Fuck you sand nigger i will find u in real life and slit your throat.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>\"\\n\\nREPLY: There is no such thing as Texas Commerce Bank of Chicago.  Likewise, there is no such thing as the United Farmers Bank of Baltimore and Albuquerque.  So Salvio, you are incorrect.  If you want to prevent even the remote possibility of confusion, then you should not be allowed to use your name, Salvio, because there may be confusion that you are related to Salvador Dali.\\n\\n\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Reply\\nHey, you could at least mention Jasenovac and 700 000 killed (not only serbs) but you say it's all bs, well, what is vandalism, death of innocent, or putting truth here?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Thats fine, there is no deadline )   chi?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"\\n\\nDYK nomination of Mustarabim\\n Hello! Your submission of Mustarabim at the Did You Know nominations page has been reviewed, and there still are some issues that may need to be clarified. Please review the comment(s) underneath  and respond there as soon as possible.  Thank you for contributing to Did You Know!   (talk • contribs) \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n",
       "0  Ahh shut the fuck up you douchebag sand nigger \\n\\nGo blow up some more people you muslim piece of shit. Fuck you sand nigger i will find u in real life and slit your throat.                                                                                                                                                                                                                          \n",
       "1  \"\\n\\nREPLY: There is no such thing as Texas Commerce Bank of Chicago.  Likewise, there is no such thing as the United Farmers Bank of Baltimore and Albuquerque.  So Salvio, you are incorrect.  If you want to prevent even the remote possibility of confusion, then you should not be allowed to use your name, Salvio, because there may be confusion that you are related to Salvador Dali.\\n\\n\"   \n",
       "2  Reply\\nHey, you could at least mention Jasenovac and 700 000 killed (not only serbs) but you say it's all bs, well, what is vandalism, death of innocent, or putting truth here?                                                                                                                                                                                                                        \n",
       "3  Thats fine, there is no deadline )   chi?                                                                                                                                                                                                                                                                                                                                                               \n",
       "4  \"\\n\\nDYK nomination of Mustarabim\\n Hello! Your submission of Mustarabim at the Did You Know nominations page has been reviewed, and there still are some issues that may need to be clarified. Please review the comment(s) underneath  and respond there as soon as possible.  Thank you for contributing to Did You Know!   (talk • contribs) \"                                                      \n",
       "\n",
       "   toxic  \n",
       "0  1      \n",
       "1  0      \n",
       "2  0      \n",
       "3  0      \n",
       "4  0      "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(PATH_TO_DATASET)\n",
    "df_bert = df.copy()\n",
    "df_bert = df_bert.sample(BERT_SAMPLES + 500, random_state = RANDOM_STATE).reset_index(drop = True) \n",
    "df_bert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функции лемматизации и очистки текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])  \n",
    "    return lemmatized_output\n",
    "\n",
    "def clear_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z']\", ' ', text)\n",
    "    return ' '.join(text.split()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим функции к нашему тексту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384ba7b88e244c3989c90f8b28d2f7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text_lemma'] = df['text'].progress_apply(lambda x: lemmatize(clear_text(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я не знаю как делать 'честную' кросс-валидацию не залезая в обучающую выборку при создании векторов, поэтому разделим данные на обучающую/валидационную и тестовую выборку в соотношении 3:1:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_all = df['text_lemma'].values.astype('U')\n",
    "target_all = df['toxic']\n",
    "\n",
    "corpus, corpus_test, target, target_test = train_test_split(\n",
    "    corpus_all, target_all, test_size=0.2, random_state = RANDOM_STATE)\n",
    "\n",
    "corpus_train, corpus_valid, target_train, target_valid = train_test_split(\n",
    "    corpus, target, test_size = 0.25, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы tf-idf: (95742, 121091)\n"
     ]
    }
   ],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words = stopwords) \n",
    "tf_idf = count_tf_idf.fit_transform(corpus_train) \n",
    "print(\"Размер матрицы tf-idf:\", tf_idf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы tf-idf_test: (31915, 121091)\n"
     ]
    }
   ],
   "source": [
    "tf_idf_test = count_tf_idf.transform(corpus_test) \n",
    "print(\"Размер матрицы tf-idf_test:\", tf_idf_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы tf-idf_valid: (31914, 121091)\n"
     ]
    }
   ],
   "source": [
    "tf_idf_valid = count_tf_idf.transform(corpus_valid) \n",
    "print(\"Размер матрицы tf-idf_valid:\", tf_idf_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпуса готовы, размерности в порядке, обучение во второй главе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим пре-тренированные токенайзер и модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Загрузка предобученной модели/токенизатора \n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем все комментарии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5dd426e4d04df8adeb4b226cf1b365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (862 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenized = df_bert['text'].progress_apply(lambda x: tokenizer.encode(x, add_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для корректной работы модели последовательность токенов (и маска) должны быть одной длины. Дополним нулями и создадим маску важных токенов.   \n",
    "Также заметим, что у модели *BERT* есть ограничение на длину последовательности токенов в 512, иначе, в дальнейшем, будет происходить ошибка индексов. Поэтому отберём только те примеры, где длина меньше 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "target = []\n",
    "for i in range(len(tokenized)):\n",
    "    if len(tokenized[i]) <= 512:\n",
    "        tokens.append(tokenized[i])\n",
    "        target.append(df_bert['toxic'][i])\n",
    "tokens = (pd.Series(tokens)).head(BERT_SAMPLES)\n",
    "target = (pd.Series(target)).head(BERT_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f170760ae645a9a4c88cb3d7612edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98ce13cafcb4a90ae448a2853b451d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tqdm(tokens.values):\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tqdm(tokens.values)])\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 511)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2000, 511)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(padded.shape, attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не забить всю память разом до отказа эмбеддинги будем создавать порционно, радуясь перемещению полоски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff167ef093949b6aab7e73f855c1175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "features_bert = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод 1-го шага:**  \n",
    "+ Мы загрузили данные, представляющие собой комментарии с разметкой о токсичности.\n",
    "+ Подготовили как TF-IDF признаки, так и ембеддинги с помощью модели *BERT*.\n",
    "+ Данные готовы к обучению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию обучения моделей и вычисления метрик. Она годится как для TF-IDF подхода, так и для эмбеддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame()\n",
    "def fit_predict_cv(model, features, target, scores, features_valid = None, target_valid = None, cv = None):\n",
    "    model = model\n",
    "    model.fit(features, target)\n",
    "    F1_valid, F1_cv = None, None\n",
    "    \n",
    "    F1_train = f1_score(target, model.predict(features))\n",
    "    \n",
    "    if cv == None:\n",
    "        F1_valid = f1_score(target_valid, model.predict(features_valid))\n",
    "    else: \n",
    "        F1_cv = cross_val_score(model, features, target, cv = cv, scoring = 'f1').mean() \n",
    "    \n",
    "    scores = scores.append({'model' : type(model).__name__,\n",
    "                            'F1_train' : F1_train,\n",
    "                            'F1_valid' : F1_valid,\n",
    "                            'F1_train_cv' : F1_cv} , ignore_index = True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. выборка несбалансированная по целевому признаку, поставим `class_weight = balanced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_train</th>\n",
       "      <th>F1_train_cv</th>\n",
       "      <th>F1_valid</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.836801</td>\n",
       "      <td>None</td>\n",
       "      <td>0.744242</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1_train F1_train_cv  F1_valid               model\n",
       "0  0.836801  None        0.744242  LogisticRegression"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = fit_predict_cv(LogisticRegression(class_weight = 'balanced'), \n",
    "                        tf_idf, target_train, scores = scores, features_valid = tf_idf_valid, target_valid = target_valid)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4089165\ttotal: 748ms\tremaining: 2m 28s\n",
      "20:\tlearn: 0.6766365\ttotal: 11.6s\tremaining: 1m 38s\n",
      "40:\tlearn: 0.7256002\ttotal: 22.4s\tremaining: 1m 26s\n",
      "60:\tlearn: 0.7530917\ttotal: 33.2s\tremaining: 1m 15s\n",
      "80:\tlearn: 0.7686712\ttotal: 43.8s\tremaining: 1m 4s\n",
      "100:\tlearn: 0.7804438\ttotal: 54.5s\tremaining: 53.4s\n",
      "120:\tlearn: 0.7866204\ttotal: 1m 5s\tremaining: 42.5s\n",
      "140:\tlearn: 0.7987213\ttotal: 1m 15s\tremaining: 31.8s\n",
      "160:\tlearn: 0.8069577\ttotal: 1m 26s\tremaining: 21s\n",
      "180:\tlearn: 0.8155759\ttotal: 1m 37s\tremaining: 10.2s\n",
      "199:\tlearn: 0.8222972\ttotal: 1m 47s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_train</th>\n",
       "      <th>F1_train_cv</th>\n",
       "      <th>F1_valid</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.836801</td>\n",
       "      <td>None</td>\n",
       "      <td>0.744242</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.822297</td>\n",
       "      <td>None</td>\n",
       "      <td>0.764283</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1_train F1_train_cv  F1_valid               model\n",
       "0  0.836801  None        0.744242  LogisticRegression\n",
       "1  0.822297  None        0.764283  CatBoostClassifier"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = fit_predict_cv(CatBoostClassifier(\n",
    "    iterations = 200, learning_rate = 0.5, eval_metric = 'F1', verbose = 20, random_state = RANDOM_STATE), \n",
    "                        tf_idf, target_train, scores = scores, features_valid = tf_idf_valid, target_valid = target_valid)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую и тестовую выборки в соотношении 75:25. Будем использовать кросс-валидацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bert = target.head(len(features_bert))\n",
    "features_train_bert, features_test_bert, target_train_bert, target_test_bert = train_test_split(\n",
    "    features_bert, target_bert, test_size = 0.25, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия c BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При количестве итераций по умолчанию (100) модель плохо сходится, увеличим это значение до `max_iter = 1000`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_train</th>\n",
       "      <th>F1_train_cv</th>\n",
       "      <th>F1_valid</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.836801</td>\n",
       "      <td>None</td>\n",
       "      <td>0.744242</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.822297</td>\n",
       "      <td>None</td>\n",
       "      <td>0.764283</td>\n",
       "      <td>CatBoostClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.678365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LogisticRegression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   F1_train F1_train_cv  F1_valid               model\n",
       "0  0.836801  None        0.744242  LogisticRegression\n",
       "1  0.822297  None        0.764283  CatBoostClassifier\n",
       "2  0.970588  0.678365   NaN        LogisticRegression"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = fit_predict_cv(LogisticRegression(max_iter = 1000), features_train_bert, target_train_bert, scores, cv = 5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод 2-го шага:**\n",
    "\n",
    "+ Мы обучили модели для предсказания токсичности комментариев, основываясь на двух подходах. Заметим, что не очень корректно сравнивать метрки между собой, т.к. для эмбедингов мы использовали всего лишь 2000 сэмплов. Однако, очевидно, преимущество в скорости TF-IDF подхода.\n",
    "+ Для подхода TF-IDF наилучшей оказалась модель catboost, со значением F1 на валидационной выборке **0.76**. Порог в 0.75 преодолен.\n",
    "+ Для подхода, основанном на эмбеддингах (для 2000 сэмплов) значение F1 на кросс-валидации составило **0.68**.\n",
    "+ Принимая во внимание имеющиеся ресурсы примем модель `catboost` подхода *TF-IDF* за лучшую модель. Протестируем ее в следующем пункте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим качество лучшей модели на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4089165\ttotal: 576ms\tremaining: 1m 54s\n",
      "20:\tlearn: 0.6766365\ttotal: 11.5s\tremaining: 1m 38s\n",
      "40:\tlearn: 0.7256002\ttotal: 22.3s\tremaining: 1m 26s\n",
      "60:\tlearn: 0.7530917\ttotal: 33.1s\tremaining: 1m 15s\n",
      "80:\tlearn: 0.7686712\ttotal: 43.9s\tremaining: 1m 4s\n",
      "100:\tlearn: 0.7804438\ttotal: 54.6s\tremaining: 53.5s\n",
      "120:\tlearn: 0.7866204\ttotal: 1m 5s\tremaining: 42.7s\n",
      "140:\tlearn: 0.7987213\ttotal: 1m 16s\tremaining: 31.9s\n",
      "160:\tlearn: 0.8069577\ttotal: 1m 26s\tremaining: 21.1s\n",
      "180:\tlearn: 0.8155759\ttotal: 1m 37s\tremaining: 10.3s\n",
      "199:\tlearn: 0.8222972\ttotal: 1m 47s\tremaining: 0us\n",
      "F1 мера лучшей модели на тестовых данных: 0.7521186440677966\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations = 200, learning_rate = 0.5, eval_metric = 'F1', verbose = 20, random_state = RANDOM_STATE)\n",
    "model.fit(tf_idf, target_train)\n",
    "print('F1 мера лучшей модели на тестовых данных:', f1_score(target_test, model.predict(tf_idf_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:**\n",
    "F1-мера для лучшей модели на тестовых данных: 0.75. Порог в 0.75 достигнут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Мы загрузили данные, представляющие собой ~160 тыс. комментариев интернет-магазин «Викишоп». \n",
    "+ Задача решалась двумя подходами:   \n",
    "    1. 'классический' **TF-IDF**  \n",
    "    2. С помощью эмбеддингов, полученных на предобученной модели **BERT**\n",
    "+ Мы подготовили признаки для двух подходов, для подхода на основе эмбеддингов использовали только 2000 сэмплов в целях экономии ресурсов.\n",
    "+ Для подхода *TF-IDF* лучшей моделью оказалась модель *catboost*, со значением F1-меры на валидациооной выборке в **0.76**, на тестовой - **0.75**. Порог задачи в 0.75 преодолен.\n",
    "+ Для подхода, основанном на эмбеддингах значение F1-меры на кросс-валидации составило **0.68** с использованием всего 2000 сэмплов, что довольно неплохо. "
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 103,
    "start_time": "2021-08-25T21:52:59.134Z"
   },
   {
    "duration": 331,
    "start_time": "2021-08-25T21:56:33.723Z"
   },
   {
    "duration": 766,
    "start_time": "2021-08-25T21:56:46.168Z"
   },
   {
    "duration": 4,
    "start_time": "2021-08-25T21:56:48.931Z"
   },
   {
    "duration": 13,
    "start_time": "2021-08-25T21:56:55.878Z"
   },
   {
    "duration": 6,
    "start_time": "2021-08-25T21:57:05.891Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "268px",
    "width": "355px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
